---
title: The long wait
categories:
- Classification
- Logistic Regression 
- Decision Tree 
- Random Forest
- AdaBoost
- Neural Network
- t-test

feature_image: /assets/img/wait/late.jpg
---
With high globalization andits rapidity, air transportation has become one of the favourite modes of transportation for many people. With this popularity comes the pain: flight delays.
 
![png](/assets/img/wait/output_58_2.png)

At the very least flight delays waste the time of multiple people and at the worst they cause environmental harm by rise in fuel consumption. Around 21% of the flights were delayed in United statesin the year 2019 as per Air travel consumer report. These costs billions of dollars loss to the airport companies and travellers. Airport management teams are therefore thinking that alerting people about flight delay prior to the journey and reducing flight delays are high priority problems to be solved in onwards scenarios. 

This motivates to formulate this business problem and find probable solution for it by learning from historical data. The focus is predicting the delays with the use of informative attributes. To carry out the predictive analysis, the model encompassed various statistical techniques and machine learning techniques.

Logistics Regression, Decision Tree, Random Forest and AdaBoost Classifier methods were applied with libraries from SciKit Learn package. A simple neural network was also constructed using the Keras Library.

Exploratory data analysis identifies cyclicity in flight delays with respect to week days (Friday with the highest rate of delays and Monday with the least) and also identifies the biggest offending airlines and airports. It also identifies which departure time is most likely to result in delays. With the exception of Decision Tree classifier which showed poorer results the rest of the predictive methods were similar in results. A precision of 0.74-0.8 is obtained in predicting whether a flight will be delayed.

The defaults parameters were applied in all the models. It's very likely that results may improve somewhat by optimizing the parameters.

The models can be already used in a real world scenario to improve the flying experience and save a lot of waste.

All the workflow can be seen below and the code is in my github repository:

* [Libraries ](#Libraries)
* [Data loading, visualization and exploratory analysis](#data)
* [Modeling](#Modeling)
    * [Baseline](#Baseline)
    * [Logistic Regression](#Logistic-Regression)
    * [Decision Tree](#Decision-Tree-Classifier)
    * [Random Forest](#Random-Forest-Classifier)
    * [AdaBoost](#AdaBoost-Classifier)
    * [Neural network model with Keras](#nn)
* [Summary](#Summary)



# Libraries <a id='Libraries'></a>


```python
#The bread and butter:
import pandas as pd
import numpy as np

#Visualizations:
import matplotlib.pyplot as plt
import seaborn as sb

#Statistics:
import scipy.stats as stats

#ML:
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_val_predict
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn import metrics
from sklearn.metrics import classification_report, roc_auc_score, precision_score,recall_score
from yellowbrick.classifier import ROCAUC
from sklearn.tree import DecisionTreeClassifier
import keras
from keras.models import Sequential
from keras.utils import to_categorical

```

# Data loading, visualization and exploratory analysis <a id='data'></a>

Data consists of all January 2019 flights downloaded from Kaggle.


```python
path19 = 'https://raw.githubusercontent.com/flikrama/UH-SPE-ML-Bootcamp-Team4-Project2/master/Jan_2019_ontime.csv'
df19 = pd.read_csv(path19)
```


```python
df19 = df19.drop('Unnamed: 21', axis = 1)
```


```python
print(df19.shape)
print(df19.describe())
print(df19.dtypes)
```

    (583985, 21)
            DAY_OF_MONTH    DAY_OF_WEEK  OP_CARRIER_AIRLINE_ID  OP_CARRIER_FL_NUM  \
    count  583985.000000  583985.000000          583985.000000      583985.000000   
    mean       15.960088       3.835626           19983.213168        2537.869334   
    std         8.987942       1.921899             377.724638        1821.736145   
    min         1.000000       1.000000           19393.000000           1.000000   
    25%         8.000000       2.000000           19790.000000         979.000000   
    50%        16.000000       4.000000           19977.000000        2114.000000   
    75%        24.000000       5.000000           20368.000000        3902.000000   
    max        31.000000       7.000000           20452.000000        7439.000000   
    
           ORIGIN_AIRPORT_ID  ORIGIN_AIRPORT_SEQ_ID  DEST_AIRPORT_ID  \
    count      583985.000000           5.839850e+05    583985.000000   
    mean        12659.701982           1.265974e+06     12659.470015   
    std          1519.405493           1.519403e+05      1519.336466   
    min         10135.000000           1.013505e+06     10135.000000   
    25%         11292.000000           1.129202e+06     11292.000000   
    50%         12889.000000           1.288903e+06     12889.000000   
    75%         13931.000000           1.393102e+06     13931.000000   
    max         16218.000000           1.621802e+06     16218.000000   
    
           DEST_AIRPORT_SEQ_ID       DEP_TIME      DEP_DEL15       ARR_TIME  \
    count         5.839850e+05  567633.000000  567630.000000  566924.000000   
    mean          1.265951e+06    1331.957814       0.174281    1484.315921   
    std           1.519334e+05     495.404020       0.379351     523.162855   
    min           1.013505e+06       1.000000       0.000000       1.000000   
    25%           1.129202e+06     921.000000       0.000000    1104.000000   
    50%           1.288903e+06    1328.000000       0.000000    1517.000000   
    75%           1.393102e+06    1738.000000       0.000000    1919.000000   
    max           1.621802e+06    2400.000000       1.000000    2400.000000   
    
               ARR_DEL15      CANCELLED       DIVERTED       DISTANCE  
    count  565963.000000  583985.000000  583985.000000  583985.000000  
    mean        0.185917       0.028641       0.002219     797.742767  
    std         0.389040       0.166796       0.047056     589.999261  
    min         0.000000       0.000000       0.000000      31.000000  
    25%         0.000000       0.000000       0.000000     363.000000  
    50%         0.000000       0.000000       0.000000     640.000000  
    75%         0.000000       0.000000       0.000000    1037.000000  
    max         1.000000       1.000000       1.000000    4983.000000  
    DAY_OF_MONTH               int64
    DAY_OF_WEEK                int64
    OP_UNIQUE_CARRIER         object
    OP_CARRIER_AIRLINE_ID      int64
    OP_CARRIER                object
    TAIL_NUM                  object
    OP_CARRIER_FL_NUM          int64
    ORIGIN_AIRPORT_ID          int64
    ORIGIN_AIRPORT_SEQ_ID      int64
    ORIGIN                    object
    DEST_AIRPORT_ID            int64
    DEST_AIRPORT_SEQ_ID        int64
    DEST                      object
    DEP_TIME                 float64
    DEP_DEL15                float64
    DEP_TIME_BLK              object
    ARR_TIME                 float64
    ARR_DEL15                float64
    CANCELLED                float64
    DIVERTED                 float64
    DISTANCE                 float64
    dtype: object
    


```python
# Number of NaN values in df for each column
df19.isna().sum()
```




    DAY_OF_MONTH                 0
    DAY_OF_WEEK                  0
    OP_UNIQUE_CARRIER            0
    OP_CARRIER_AIRLINE_ID        0
    OP_CARRIER                   0
    TAIL_NUM                  2543
    OP_CARRIER_FL_NUM            0
    ORIGIN_AIRPORT_ID            0
    ORIGIN_AIRPORT_SEQ_ID        0
    ORIGIN                       0
    DEST_AIRPORT_ID              0
    DEST_AIRPORT_SEQ_ID          0
    DEST                         0
    DEP_TIME                 16352
    DEP_DEL15                16355
    DEP_TIME_BLK                 0
    ARR_TIME                 17061
    ARR_DEL15                18022
    CANCELLED                    0
    DIVERTED                     0
    DISTANCE                     0
    dtype: int64




```python
# dropped NaN values in df
df19 = df19.dropna()
print(df19.shape)
```

    (565963, 21)
    


```python
# Number of Unique elements in df
df19.nunique()
```




    DAY_OF_MONTH               31
    DAY_OF_WEEK                 7
    OP_UNIQUE_CARRIER          17
    OP_CARRIER_AIRLINE_ID      17
    OP_CARRIER                 17
    TAIL_NUM                 5441
    OP_CARRIER_FL_NUM        6833
    ORIGIN_AIRPORT_ID         346
    ORIGIN_AIRPORT_SEQ_ID     346
    ORIGIN                    346
    DEST_AIRPORT_ID           346
    DEST_AIRPORT_SEQ_ID       346
    DEST                      346
    DEP_TIME                 1438
    DEP_DEL15                   2
    DEP_TIME_BLK               19
    ARR_TIME                 1440
    ARR_DEL15                   2
    CANCELLED                   1
    DIVERTED                    1
    DISTANCE                 1451
    dtype: int64




```python
#Remove columns which won't be used
df_red = df19.drop([ 'OP_CARRIER_AIRLINE_ID', 'OP_CARRIER', 'TAIL_NUM', 'OP_CARRIER_FL_NUM',
       'ORIGIN_AIRPORT_ID', 'ORIGIN_AIRPORT_SEQ_ID', 'DEST_AIRPORT_ID', 'DEST_AIRPORT_SEQ_ID', 'DEP_TIME',
       'CANCELLED','ARR_TIME',
       'DIVERTED'], axis = 1)
```


```python
df_red.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>DAY_OF_MONTH</th>
      <th>DAY_OF_WEEK</th>
      <th>OP_UNIQUE_CARRIER</th>
      <th>ORIGIN</th>
      <th>DEST</th>
      <th>DEP_DEL15</th>
      <th>DEP_TIME_BLK</th>
      <th>ARR_DEL15</th>
      <th>DISTANCE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>9E</td>
      <td>GNV</td>
      <td>ATL</td>
      <td>0.0</td>
      <td>0600-0659</td>
      <td>0.0</td>
      <td>300.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>2</td>
      <td>9E</td>
      <td>MSP</td>
      <td>CVG</td>
      <td>0.0</td>
      <td>1400-1459</td>
      <td>0.0</td>
      <td>596.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>2</td>
      <td>9E</td>
      <td>DTW</td>
      <td>CVG</td>
      <td>0.0</td>
      <td>1200-1259</td>
      <td>0.0</td>
      <td>229.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>2</td>
      <td>9E</td>
      <td>TLH</td>
      <td>ATL</td>
      <td>0.0</td>
      <td>1500-1559</td>
      <td>0.0</td>
      <td>223.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>2</td>
      <td>9E</td>
      <td>ATL</td>
      <td>FSM</td>
      <td>0.0</td>
      <td>1900-1959</td>
      <td>0.0</td>
      <td>579.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
#all delayed flights by weekday
sb.countplot(x = df_red['ARR_DEL15'], hue = df_red['DAY_OF_WEEK'])
```




![png](/assets/img/wait/output_12_1.png)



```python
#all flights per day
f, ax = plt.subplots()
sb.countplot(x = df_red['DAY_OF_WEEK'])
f.set_size_inches(4,5)
```


![png](/assets/img/wait/output_13_0.png)


Seems like we need to normalize the delayed flights per day to the total number of flights per day


```python
from matplotlib.pyplot import figure
figure(num=None, figsize=(8, 6))
plt.bar(x = df_red['DAY_OF_WEEK'].unique(), height = df_red.groupby(['DAY_OF_WEEK'])['ARR_DEL15'].mean())
plt.ylim([0, 0.25])
plt.xlabel('Day of week')
plt.ylabel('Fraction of flights delayed')
```

![png](/assets/img/wait/output_15_1.png)


As can be seen the fraction of flights delayed per day looks different from the total delayed flights. Highest proportion is day 5 as opposed to day 4 in raw total numbers


```python
figure(num=None, figsize=(18, 6))
plt.bar(x = df_red['DAY_OF_MONTH'].unique(), height = df_red.groupby(['DAY_OF_MONTH']).mean()['ARR_DEL15'])
plt.ylim([0, 0.35])
plt.xlabel('Day of month')
plt.ylabel('Fraction of flights delayed')
```



![png](/assets/img/wait/output_17_1.png)



```python
figure(num=None, figsize=(18, 6))
plt.bar(x = df_red['OP_UNIQUE_CARRIER'].unique(), height = df_red.groupby(['OP_UNIQUE_CARRIER']).mean()['ARR_DEL15'])
plt.ylim([0, 0.3])
plt.xlabel('Carrier')
plt.ylabel('Fraction of flights delayed')
```



![png](/assets/img/wait/output_18_1.png)



```python
figure(num=None, figsize=(24, 6))
plt.bar(x = df_red['DEP_TIME_BLK'].unique(), height = df_red.groupby(['DEP_TIME_BLK']).mean()['ARR_DEL15'])
plt.ylim([0., 0.25])
plt.xlabel('Departure Time')
plt.ylabel('Fraction of flights delayed')
```


![png](/assets/img/wait/output_19_1.png)


List below shows what departure time results in most delays proportionally--it can be seen that it's the 1800-18:59 time slot:


```python
df_red.groupby(['DEP_TIME_BLK']).mean()['ARR_DEL15'].sort_values()
```




    DEP_TIME_BLK
    0600-0659    0.112813
    0001-0559    0.118162
    0700-0759    0.127854
    0800-0859    0.145400
    0900-0959    0.152186
    2300-2359    0.162692
    1000-1059    0.176263
    1100-1159    0.177826
    2200-2259    0.184900
    1200-1259    0.185168
    1300-1359    0.196801
    1500-1559    0.208362
    1400-1459    0.209567
    2100-2159    0.214907
    1600-1659    0.221165
    2000-2059    0.228903
    1700-1759    0.229099
    1900-1959    0.234680
    1800-1859    0.239034
    Name: ARR_DEL15, dtype: float64



Lowest and Highest ratio of delayed flights by Destination shown below; do not fly to Ogden city as you have a 50% chance of being late:


```python
df_red.groupby(['DEST']).mean()['ARR_DEL15'].sort_values()
```




    DEST
    ELM    0.000000
    CPR    0.044118
    LWS    0.047619
    DRT    0.050847
    BTM    0.051724
             ...   
    HGR    0.400000
    MKG    0.425532
    MMH    0.440000
    OGS    0.500000
    OGD    0.500000
    Name: ARR_DEL15, Length: 346, dtype: float64



Lowest and Highest ratio of delayed flights by Origin shown below; Ogden City again stands out:


```python
#delayed_norm['ORIGIN'] = df_red['ORIGIN']
df_red.groupby(['ORIGIN']).mean()['ARR_DEL15'].sort_values()
```




    ORIGIN
    LYH    0.000000
    PPG    0.000000
    CPR    0.014706
    LWS    0.031250
    BRW    0.033333
             ...   
    ACV    0.429825
    VEL    0.434783
    OWB    0.444444
    OGD    0.500000
    OGS    0.600000
    Name: ARR_DEL15, Length: 346, dtype: float64



Let us check whether delayed departure translates to also delayed arrival:


```python
#Check by delay in departure
sb.countplot(x = df_red['ARR_DEL15'], hue = df_red['DEP_DEL15'])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2828db35340>




![png](/assets/img/wait/output_27_1.png)


As expected, flights that leave late also (mostly) arrive late. Let us now look at distance traveled below:


```python
sb.boxplot('DEP_DEL15', 'DISTANCE', data=df_red, orient='v')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2828db7cb80>




![png](/assets/img/wait/output_29_1.png)



```python
df_red.groupby(['DEP_DEL15']).mean()['DISTANCE']
```




    DEP_DEL15
    0.0    799.301073
    1.0    817.114257
    Name: DISTANCE, dtype: float64




```python
df_red.groupby(['DEP_DEL15']).median()['DISTANCE']
```




    DEP_DEL15
    0.0    637.0
    1.0    674.0
    Name: DISTANCE, dtype: float64



There seems to be a slightly higher distance traveled for flights that were delayed. Let us do a t-test to check whether this difference is statistically significant


```python
stats.ttest_ind(df_red[df_red['ARR_DEL15'] == 1]['DISTANCE'], df_red[df_red['ARR_DEL15'] == 0]['DISTANCE'], equal_var=False)
```




    Ttest_indResult(statistic=2.5804047623776594, pvalue=0.009869350308986821)



It is significant!

Finally dummyfying variables for categorical variables: OP_UNIQUE_CARRIER', 'ORIGIN', 'DEST','DEP_TIME_BLK', 'DAY_OF_MONTH', 'DAY_OF_WEEK and normalizing the 'DISTANCE


```python
df_dum = pd.get_dummies(df_red, columns = ['ORIGIN', 
                                           'DEST', 
                                           'OP_UNIQUE_CARRIER', 
                                           'DEP_TIME_BLK', 
                                           'DAY_OF_MONTH',
                                            'DAY_OF_WEEK'])
```


```python
mean = df_dum['DISTANCE'].mean()
std =  df_dum['DISTANCE'].std()
df_dum['DISTANCE'] = (df_dum['DISTANCE'] - mean)/std
```


```python
df_dum.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>DEP_DEL15</th>
      <th>ARR_DEL15</th>
      <th>DISTANCE</th>
      <th>ORIGIN_ABE</th>
      <th>ORIGIN_ABI</th>
      <th>ORIGIN_ABQ</th>
      <th>ORIGIN_ABR</th>
      <th>ORIGIN_ABY</th>
      <th>ORIGIN_ACT</th>
      <th>ORIGIN_ACV</th>
      <th>...</th>
      <th>DAY_OF_MONTH_29</th>
      <th>DAY_OF_MONTH_30</th>
      <th>DAY_OF_MONTH_31</th>
      <th>DAY_OF_WEEK_1</th>
      <th>DAY_OF_WEEK_2</th>
      <th>DAY_OF_WEEK_3</th>
      <th>DAY_OF_WEEK_4</th>
      <th>DAY_OF_WEEK_5</th>
      <th>DAY_OF_WEEK_6</th>
      <th>DAY_OF_WEEK_7</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>-0.848173</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>-0.348448</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>-0.968039</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>-0.978169</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>-0.377149</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 769 columns</p>
</div>



# Modeling <a id='Modeling'></a>


## Baseline <a id='Baseline'></a>

Let us first estimate what a completely random estimate would give us (an estimate that does not rely on data science or fancy machine learning at all)
That is to say if we randomly guessed whether a flight would be late or not what our accuracy would be? Let's first get the proportion of flights that are late:


```python
df_dum[df_dum['ARR_DEL15']==1].shape[0]/df_dum.shape[0]
```




    0.18591674720785634



So if we randomly guessed a flight would be late we'd be correct 18.6% of the time; if we randomly guessed a flight would not be late then we'd be correct 81.4% of the time. Let's check whether our models beat this

## Logistic Regression <a id='Logistic-Regression'></a>


```python
X = df_dum.drop(['ARR_DEL15'], axis = 1)
y = df_dum['ARR_DEL15']
```


```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5) # 50% training and 50% test
```


```python
lr_model = LogisticRegression(max_iter=1000, tol = 0.001,  verbose=1)
lr_model.fit(X_train, y_train)
```

    [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
    [Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.9min finished
    




    LogisticRegression(max_iter=1000, tol=0.001, verbose=1)




```python
cv = StratifiedKFold(n_splits=3, shuffle=True)
```


```python
import time

time0 = time.time()
result = cross_val_score(lr_model,X_train,y_train, cv = cv)
time1 = time.time()

print('code took ' + str(time1 - time0) + ' seconds')
```

    [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
    [Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished
    [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
    [Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min finished
    [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
    [Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished
    

    code took 319.75836396217346 seconds
    


```python
print(f'Lower Limit: {np.min(result)}')
print(f'Mean: {np.mean(result)}')
print(f'Upper Limit: {np.max(result)}')
```

    Lower Limit: 0.9151144423123814
    Mean: 0.9163866125287563
    Upper Limit: 0.9170969075662324
    


```python
pred = lr_model.predict(X_test)
pred_prob = lr_model.predict_proba(X_test)
```


```python
print("Classification:\n",classification_report(y_test, pred, digits=3))

# print the area under the curve
print(f'AUC: {roc_auc_score(y_test,pred_prob[:,1])}')
```

    Classification:
                   precision    recall  f1-score   support
    
             0.0      0.942     0.957     0.950    230388
             1.0      0.799     0.742     0.769     52594
    
        accuracy                          0.917    282982
       macro avg      0.870     0.850     0.859    282982
    weighted avg      0.915     0.917     0.916    282982
    
    AUC: 0.8952169462174631
    


```python
ax = plt.axes()
visualizer = ROCAUC(lr_model, 
                    ax = ax,
                    classes = ['Not Delayed', 'Delayed'])

visualizer.fit(X_train, y_train)         
visualizer.score(X_test, y_test)
ax.set_title('Logistic Regression Model')
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.legend()
```

    c:\python37\lib\site-packages\sklearn\base.py:209: FutureWarning:
    
    From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.
    
    




    <matplotlib.legend.Legend at 0x282b875b100>




![png](/assets/img/wait/output_50_2.png)


## Decision Tree Classifier <a id='Decision-Tree-Classifier'></a>


```python
clf_dt = DecisionTreeClassifier(random_state = 0)
dt_model = clf_dt.fit(X_train, y_train)
```


```python
y_pred = dt_model.predict(X_test)
print(metrics.classification_report(y_test,y_pred))
```

                  precision    recall  f1-score   support
    
             0.0       0.93      0.93      0.93    230388
             1.0       0.70      0.67      0.68     52594
    
        accuracy                           0.88    282982
       macro avg       0.81      0.80      0.81    282982
    weighted avg       0.88      0.88      0.88    282982
    
    


```python
ax = plt.axes()
visualizer = ROCAUC(dt_model, 
                    ax = ax,
                    per_class = True,
                    classes = ['Not Delayed', 'Delayed'])

visualizer.fit(X_train, y_train)         
visualizer.score(X_test, y_test) 
ax.set_title('Decision Tree Model')
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.legend()
```

    c:\python37\lib\site-packages\sklearn\base.py:209: FutureWarning:
    
    From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.
    
    




    <matplotlib.legend.Legend at 0x2828f44a0a0>




![png](/assets/img/wait/output_54_2.png)


## Random Forest Classifier <a id='Random-Forest-Classifier'></a>


```python
clf_rf=RandomForestClassifier(n_estimators=10)
rf_model = clf_rf.fit(X_train,y_train)
y_pred=rf_model.predict(X_test)
```


```python
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
```

    Accuracy: 0.907244983779887
    


```python
ax = plt.axes()
visualizer = ROCAUC(rf_model, 
                    ax = ax,
                    classes = ['Not Delayed', 'Delayed'])

visualizer.fit(X_train, y_train)         
visualizer.score(X_test, y_test) 
ax.set_title('Random Forest Model')
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.legend()
```

    c:\python37\lib\site-packages\sklearn\base.py:209: FutureWarning:
    
    From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.
    
    




    <matplotlib.legend.Legend at 0x2828f4a2ac0>




![png](/assets/img/wait/output_58_2.png)


## AdaBoost Classifier <a id='AdaBoost-Classifier'></a>



```python
Adaboost = AdaBoostClassifier()
model_adaboost = Adaboost.fit(X_train, y_train)
```


```python
from sklearn import metrics
y_pred = model_adaboost.predict(X_test)
print(metrics.classification_report(y_test,y_pred))
```

                  precision    recall  f1-score   support
    
             0.0       0.94      0.96      0.95    230388
             1.0       0.80      0.74      0.77     52594
    
        accuracy                           0.92    282982
       macro avg       0.87      0.85      0.86    282982
    weighted avg       0.92      0.92      0.92    282982
    
    


```python
ax = plt.axes()
visualizer = ROCAUC(model_adaboost, 
                    ax = ax,
                    classes = ['Not Delayed', 'Delayed'])

visualizer.fit(X_train, y_train)         
visualizer.score(X_test, y_test) 
ax.set_title('Adaboost Model')
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.legend()
```

    c:\python37\lib\site-packages\sklearn\base.py:209: FutureWarning:
    
    From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.
    
    




    <matplotlib.legend.Legend at 0x28290126eb0>




![png](/assets/img/wait/output_62_2.png)


## Neural network model with Keras  <a id='nn'></a>


```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)


nn_model_s = Sequential()
nn_model_s.add(Dense(64, input_dim= X_train.shape[1], activation= 'relu'))
nn_model_s.add(Dense(8, input_dim= X_train.shape[1], activation= 'relu'))
nn_model_s.add(Dense(2, activation= 'softmax'))

nn_model_s.compile(loss= 'binary_crossentropy', 
              #optimizer= opt, 
              metrics= ['accuracy'])
history_s = nn_model_s.fit(X_train, y_train,validation_split = 0.5, epochs= 20, batch_size=128)


```

    Epoch 1/20
    1106/1106 [==============================] - 18s 16ms/step - loss: 0.2767 - accuracy: 0.9074 - val_loss: 0.2597 - val_accuracy: 0.9162
    Epoch 2/20
    1106/1106 [==============================] - 8s 8ms/step - loss: 0.2586 - accuracy: 0.9160 - val_loss: 0.2580 - val_accuracy: 0.9165
    Epoch 3/20
    1106/1106 [==============================] - 9s 8ms/step - loss: 0.2562 - accuracy: 0.9160 - val_loss: 0.2563 - val_accuracy: 0.9170
    Epoch 4/20
    1106/1106 [==============================] - 9s 8ms/step - loss: 0.2539 - accuracy: 0.9162 - val_loss: 0.2553 - val_accuracy: 0.9168
    Epoch 5/20
    1106/1106 [==============================] - 9s 8ms/step - loss: 0.2515 - accuracy: 0.9164 - val_loss: 0.2579 - val_accuracy: 0.9165
    Epoch 6/20
    1106/1106 [==============================] - 9s 8ms/step - loss: 0.2493 - accuracy: 0.9165 - val_loss: 0.2530 - val_accuracy: 0.9168
    Epoch 7/20
    1106/1106 [==============================] - 9s 8ms/step - loss: 0.2471 - accuracy: 0.9166 - val_loss: 0.2518 - val_accuracy: 0.9167
    Epoch 8/20
    1106/1106 [==============================] - 10s 9ms/step - loss: 0.2456 - accuracy: 0.9168 - val_loss: 0.2518 - val_accuracy: 0.9166
    Epoch 9/20
    1106/1106 [==============================] - 9s 9ms/step - loss: 0.2439 - accuracy: 0.9171 - val_loss: 0.2522 - val_accuracy: 0.9158
    Epoch 10/20
    1106/1106 [==============================] - 9s 8ms/step - loss: 0.2423 - accuracy: 0.9175 - val_loss: 0.2523 - val_accuracy: 0.9161
    Epoch 11/20
    1106/1106 [==============================] - 9s 8ms/step - loss: 0.2415 - accuracy: 0.9174 - val_loss: 0.2526 - val_accuracy: 0.9156
    Epoch 12/20
    1106/1106 [==============================] - 9s 8ms/step - loss: 0.2403 - accuracy: 0.9179 - val_loss: 0.2578 - val_accuracy: 0.9156
    Epoch 13/20
    1106/1106 [==============================] - 9s 8ms/step - loss: 0.2394 - accuracy: 0.9183 - val_loss: 0.2583 - val_accuracy: 0.9127
    Epoch 14/20
    1106/1106 [==============================] - 9s 8ms/step - loss: 0.2380 - accuracy: 0.9185 - val_loss: 0.2523 - val_accuracy: 0.9154
    Epoch 15/20
    1106/1106 [==============================] - 9s 8ms/step - loss: 0.2373 - accuracy: 0.9186 - val_loss: 0.2621 - val_accuracy: 0.9145
    Epoch 16/20
    1106/1106 [==============================] - 8s 8ms/step - loss: 0.2366 - accuracy: 0.9188 - val_loss: 0.2556 - val_accuracy: 0.9151
    Epoch 17/20
    1106/1106 [==============================] - 9s 8ms/step - loss: 0.2359 - accuracy: 0.9184 - val_loss: 0.2524 - val_accuracy: 0.9151
    Epoch 18/20
    1106/1106 [==============================] - 8s 7ms/step - loss: 0.2352 - accuracy: 0.9189 - val_loss: 0.2573 - val_accuracy: 0.9135
    Epoch 19/20
    1106/1106 [==============================] - 8s 7ms/step - loss: 0.2344 - accuracy: 0.9192 - val_loss: 0.2537 - val_accuracy: 0.9150
    Epoch 20/20
    1106/1106 [==============================] - 8s 7ms/step - loss: 0.2336 - accuracy: 0.9192 - val_loss: 0.2561 - val_accuracy: 0.9145
    


```python
f, (ax1, ax2) = plt.subplots(2,1,sharex= True)
ax1.set_title('Loss')
ax1.plot(history_s.history['loss'], label='train')
ax1.plot(history_s.history['val_loss'], label='validation')
ax1.legend()
# plot accuracy during training
ax2.set_title('Accuracy')
ax2.set_xlabel('Epoch #')
ax2.plot(history_s.history['accuracy'], label='train')
ax2.plot(history_s.history['val_accuracy'], label='validation')
ax2.legend()
plt.show()
```


![png](/assets/img/wait/output_65_0.png)



```python
_, train_acc = nn_model_s.evaluate(X_train, y_train, verbose=0)
_, test_acc = nn_model_s.evaluate(X_test, y_test, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))
```

    Train: 0.918, Test: 0.915
    

Overfitting starts almost immediately however it does not make the prediction accuracy much worse on the validation dataset. 2 epochs are recommended for the model to train


```python
y_pred = nn_model_s.predict(X_test)
y_pred  = (y_pred > 0.5)
print(metrics.classification_report(y_test,y_pred))
```

                  precision    recall  f1-score   support
    
               0       0.94      0.95      0.95    230687
               1       0.79      0.74      0.76     52295
    
       micro avg       0.91      0.91      0.91    282982
       macro avg       0.86      0.85      0.85    282982
    weighted avg       0.91      0.91      0.91    282982
     samples avg       0.91      0.91      0.91    282982
    
    

## Summary <a id='Summary'></a>

Exploratory data analysis identifies cyclicity in flight delays with respect to week days and also identifies the biggest offending airlines and airports. It also identifies which departure time is most likely to result in delays.

With the exception of Decision Tree classifier which showed poorer results the rest of the predictive methods were similar in results. A precision of 0.74-0.8 is obtained in predicting whether a flight will be delayed or not.

The defaults parameters were applied in all the models. It's very likely that results may improve somewhat by optimizing them.


{% include button.html text="Github" icon="github" link="https://github.com/flikrama" color="#0366d6" %} {% include button.html text="Linkedin" icon="linkedin" link="https://www.linkedin.com/in/likrama/" color="#0e76a8" %}   [**Resume**](/assets/resume/Fatmir_Likrama.pdf)